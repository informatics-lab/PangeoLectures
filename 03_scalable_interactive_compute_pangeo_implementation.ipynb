{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expanded-street",
   "metadata": {},
   "source": [
    "# Section 3: The Pangeo Approach - An Implementation Guide\n",
    "![Pangeo Logo](images/pangeo_logo_small.png)\n",
    "\n",
    "[Pangeo Website](https://pangeo.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-heavy",
   "metadata": {},
   "source": [
    "![A stack of technoogies](images/pangeoStackElements_buildYourOwn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-assembly",
   "metadata": {},
   "source": [
    "In this section we will look at specific tools that allow you to build your Pangeo Implementation. As the diagram shows, a specific implementation of the Pangeo does not contain a specific stack of tools, or even one of a selection of specific monolithic stacks. Rather there are certain categories for which a specific tool must be choose, but each of the tools shjould be able to be swapped in and out depending on the requirements of the specific implementation. The categories of tookls include:\n",
    "* *Compute platform* -  Where will the actual computation take place. Options - HPC, public cloud provider (AWS, Azure, Alibaba, Digital Ocean), private cloud (European Weather Cloud), Cluster, Local Machine\n",
    "  * Compute mode - how will compute be triggered. Options - Interactiver notebooks, batch scheduler, serverless architecture (e.g. AWS lambda)\n",
    "  * How will my compute be scaled elastically? Options - kubernetes, AWS ECS, Dask Cloud Provider\n",
    "  *\n",
    "* *Data storage* - Where will the data be stored? Options - Distributed cloud storage, Relational Database, Data warehouse\n",
    "  * Data format - What format will the data be stored in? Options - NetCDF, CSV, RDS, zarr, TileDB\n",
    "  * Data model - What will handle interpreting data and metadata as a cohesive data model?\n",
    "  * Data Arrays - What will handle the raw processing of arrays of numbers? Options: dask, numpy\n",
    "* *Interaction* - How will I interact with the compute and data? Options - Jupyter notebook, dashboard website\n",
    "* *Environment management* - What will create the software environment for my research? Options: conda, pip, docker containers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-stability",
   "metadata": {},
   "source": [
    "## Cloud computing\n",
    "\n",
    "The advent of cloud computing services allows us to provision the compute services we need for scientific in a completely different way, and one that is more suitable for the sort of workflows and expertise that we can expect an average researcher to have. We can use the different core services of the cloud providers for the different elements of Pangeo system in different ways. Although you can generally swap different elements in and out of the stack of a particular Pangeo implementation, you generally need to choose one platform provider e.g. AWS, Azure etc. as it is either necesarry or optimal for these to work together. We will now take a look at what cloud services we might use in our stack. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-african",
   "metadata": {},
   "source": [
    "### Low level services\n",
    "\n",
    "When setting up a compute platform we start by thinking in terms of low-level components of CPUs and storage space. All major *cloud service providers* (CSPs) have similar comparable offerings in this space. The ability to easily provision computing resources like is often called **infrastructure as a service**.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Infrastructure_as_a_service\n",
    "\n",
    "The table below shows the names of the comparable services on different platforms.\n",
    " \n",
    " \n",
    "Service / Provider | AWS | Azure | GCP \n",
    "--- |--- |--- |---  \n",
    "Compute (VM) | EC2 | Azure VM | Compute Engine\n",
    "Object Storage | S3 | Blob Storage | Cloud Storage\n",
    "\n",
    "Comparison of offerings: http://comparecloud.in\n",
    "\n",
    "Our Pangeo implementation will use the APIs provided by CSPs to quickly obtain the resources needed to spin up our platform and configure them for appropriate access, interoperability and security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-prime",
   "metadata": {},
   "source": [
    "### High-level service - Platform as a Service etc.\n",
    "\n",
    "As the offerings from CSP have developed, new more specialised services have been created. Using low-level services, users have to set up all aspects of the environment for their particular application, choosing appropriate configurations for sharing, security, reliability etc. Usually this means specialised software engineers or infrastructure engineers to make this happen. For a large organisation, there are sufficient people and skills to maintain the goal of separation of concerns, but this is not true for smaller groups and organisations. Instead, one can use higher level services where the technical details are taken care of. Increasingly higher-level service components, sch as data warehousing and machine learning platforms where low-level configuration is taken care of, are being part of the software stack for Pangeo implementations. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Platform_as_a_service\n",
    "\n",
    "Service / Provider | AWS | Azure | GCP \n",
    "--- |--- |--- |---  \n",
    "Machine Learning | Sagemaker | AzureML | DataLab / Cloud AutoML\n",
    "Database | RDS | Azure SQQL DB | Cloud SQL\n",
    "Data Warehouse | \n",
    "Query aaS | Athena | Data Lake Analytics | BigQuery\n",
    "\n",
    "We also have 3rd party providers of these higher level services, building value-added layers on the low-level infrastrucutre of major CSPs to deliver specialised services, for example database solutions ([TileDB](https://tiledb.com/) or [MongoDB](https://www.mongodb.com/cloud))  or machine learning platforms ([Determined AI](https://determined.ai/enterprise/)\n",
    "\n",
    "CSPs liken the development of cloud computing to the development of an integrated grid for electricity distribution. In the early days of electricity each factory had their own generators and required expertise in electrical engineering. With a electricity grid, central suppliers provide the electricity and the associated expertise to run it. This is the direction that computing is going in. The trade-off is that higher level services often are less portable resulting in vendor lock-in. So we balance the convenience of higher-level services in our Pangeo implementation with the goals of reproducible, shareable research which favour open-source tools deployed on low-level services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-priority",
   "metadata": {},
   "source": [
    "### More information on cloud providers\n",
    "\n",
    "* AWS\n",
    "* Azure\n",
    "* Digital Ocean\n",
    "\n",
    "Comparison of cloud providers: https://www.varonis.com/blog/aws-vs-azure-vs-google/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-postage",
   "metadata": {},
   "source": [
    "## Creating and sharing the tool stack\n",
    "\n",
    "One of the challenges of computing platforms is setting up the right environment of tools and libraries to support the scientific research being done, while avoiding this task consuming all the researcher's time. There have been substantial developments in this space which make this task easier and support the goals of reproducible and shareable research and aid in separation of concerns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-baghdad",
   "metadata": {},
   "source": [
    "### Environment managers - pip and conda\n",
    "\n",
    "Particularly in the python ecosystem, tools such as *pip* and *conda* allow one to specify the tools to deploy on a particular compute instance as a file, allowing an **infrastructure as code** (IaC) approach to tools.  As with cloud resource provision, complete specification of the configuration as a file allows others to reproduce the environment and thus reproduce the scientific research. This is not always yet as easy and trouble free as we would hope, but these tools have gone a long way towards this goal and are often used as part of a Pangeo implemntation to configure the research environment.\n",
    "\n",
    "Additional info\n",
    "* pip https://pip.pypa.io/en/stable/\n",
    "* conda https://docs.conda.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-philippines",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n",
    "Another similar tool is *containers*. These are essentially lightweight virtual machines intended for running a single task efficiently and at scale. As with environemnt managers, you completely configure an *image* through a cofiguration which specifies what should be installed and configured inside the container. You then build a particular *instance* of your container from the image. One can build hundred of instances to run in parallel. Compute jobs can then be distirbuted among these containers at a task or distribution level, to make use of the massively parallel, distributed nature of the compute and data storage infrastructure. Over time, repositories of ready made containers have built up, so a researcher should not need to do much configuration to get started.\n",
    "\n",
    "\n",
    "\n",
    "Additional information\n",
    "* docker: library for creating and running containers - https://docker.com \n",
    "* Docker Hub: library of ready to use containers - https://hub.docker.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-australia",
   "metadata": {},
   "source": [
    "## Orchestration\n",
    "\n",
    "The challenge in distributed computing is always getting the many individual workers to coordinate the work they are doing. Before they start doing any actual work, the cluster of workers must be set up appropriately from the cloud resourced we have requested to enable this inter-task communication. This is the job of orchestration software. Once again, we use an Infrastructure as code apporach to specify how many workers we want and how they should be configured and the orchestration software then acquires and sets up the resources, such as cloud VMs running containers.\n",
    "\n",
    "Additional info:\n",
    "* Kubernetes - https://kubernetes.io/\n",
    "* AWS Elastic Container Service - https://aws.amazon.com/ecs/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc\n",
    "* Azure Container services - https://azure.microsoft.com/en-gb/product-categories/containers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-thong",
   "metadata": {},
   "source": [
    "## Task Distribution\n",
    "\n",
    "With our compute cluster set up and running, we then need a way to execute our tasks in a distributed fashion. We need a way to handling splitting our large dataset into sub-domains where a particular operation can be performed separately on each sub-domain or a separate compute worker. One library that does this is dask.\n",
    "\n",
    "Additional Information:\n",
    "* dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-cincinnati",
   "metadata": {},
   "source": [
    "introduction to dask\n",
    "\n",
    "discussion of task graph and data level parallelism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recent-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo code: doing some analysis with a dask kubernetes cluster on a pangeo instance\n",
    "# we will actually need a pangeo instance to demo this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-warrant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "psychological-potato",
   "metadata": {},
   "source": [
    "## Interactivity & portability - Jupyter Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-mouse",
   "metadata": {},
   "source": [
    "Introduction to Jupyer labs\n",
    "\n",
    "(use material from pythobn guild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo running compute (again)\n",
    "#demo accessing data in notebook\n",
    "# demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-happiness",
   "metadata": {},
   "source": [
    "## Dashboarding\n",
    "\n",
    "What is a dashboard\n",
    "introduction to bokeh for interactive visualisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "requested-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo bokeh, holoview in a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-taxation",
   "metadata": {},
   "source": [
    "## Existing example installations \n",
    "\n",
    "Pangeo has been installed in many different places around the world\n",
    "* Informatics Lab research deployment - AWS, Azure\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-federation",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "commercial-journalist",
   "metadata": {},
   "source": [
    "Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-physiology",
   "metadata": {},
   "source": [
    "Deployment guide: https://pangeo.io/setup_guides/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-taxation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
