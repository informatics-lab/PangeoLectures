{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minor-boost",
   "metadata": {},
   "source": [
    "# Section 3: The Pangeo Approach - An Implementation Guide\n",
    "![Pangeo Logo](images/pangeo_logo_small.png)\n",
    "\n",
    "[Pangeo Website](https://pangeo.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-pencil",
   "metadata": {},
   "source": [
    "![A stack of technoogies](images/pangeoStackElements_buildYourOwn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-genesis",
   "metadata": {},
   "source": [
    "In this section we will look at specific tools that allow you to build your Pangeo Implementation. As the diagram shows, a specific implementation of the Pangeo does not contain a specific stack of tools, or even one of a selection of specific monolithic stacks. Rather there are certain categories for which a specific tool must be choose, but each of the tools shjould be able to be swapped in and out depending on the requirements of the specific implementation. The categories of tookls include:\n",
    "* *Compute platform* -  Where will the actual computation take place. Options - HPC, public cloud provider (AWS, Azure, Alibaba, Digital Ocean), private cloud (European Weather Cloud), Cluster, Local Machine\n",
    "  * Compute mode - how will compute be triggered. Options - Interactiver notebooks, batch scheduler, serverless architecture (e.g. AWS lambda)\n",
    "  * How will my compute be scaled elastically? Options - kubernetes, AWS ECS, Dask Cloud Provider\n",
    "  *\n",
    "* *Data storage* - Where will the data be stored? Options - Distributed cloud storage, Relational Database, Data warehouse\n",
    "  * Data format - What format will the data be stored in? Options - NetCDF, CSV, RDS, zarr, TileDB\n",
    "  * Data model - What will handle interpreting data and metadata as a cohesive data model?\n",
    "  * Data Arrays - What will handle the raw processing of arrays of numbers? Options: dask, numpy\n",
    "* *Interaction* - How will I interact with the compute and data? Options - Jupyter notebook, dashboard website\n",
    "* *Environment management* - What will create the software environment for my research? Options: conda, pip, docker containers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-coordinate",
   "metadata": {},
   "source": [
    "## Cloud computing\n",
    "\n",
    "The advent of cloud computing services allows us to provision the compute services we need for scientific in a completely different way, and one that is more suitable for the sort of workflows and expertise that we can expect an average researcher to have. We can use the different core services of the cloud providers for the different elements of Pangeo system in different ways. Although you can generally swap different elements in and out of the stack of a particular Pangeo implementation, you generally need to choose one platform provider e.g. AWS, Azure etc. as it is either necesarry or optimal for these to work together. We will now take a look at what cloud services we might use in our stack. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-question",
   "metadata": {},
   "source": [
    "### Low level services\n",
    "\n",
    "When setting up a compute platform we start by thinking in terms of low-level components of CPUs and storage space. All major *cloud service providers* (CSPs) have similar comparable offerings in this space. The ability to easily provision computing resources like is often called **infrastructure as a service**.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Infrastructure_as_a_service\n",
    "\n",
    "The table below shows the names of the comparable services on different platforms.\n",
    " \n",
    " \n",
    "Service / Provider | AWS | Azure | GCP \n",
    "--- |--- |--- |---  \n",
    "Compute (VM) | EC2 | Azure VM | Compute Engine\n",
    "Object Storage | S3 | Blob Storage | Cloud Storage\n",
    "\n",
    "Comparison of offerings: http://comparecloud.in\n",
    "\n",
    "Our Pangeo implementation will use the APIs provided by CSPs to quickly obtain the resources needed to spin up our platform and configure them for appropriate access, interoperability and security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-lawsuit",
   "metadata": {},
   "source": [
    "### High-level service - Platform as a Service etc.\n",
    "\n",
    "As the offerings from CSP have developed, new more specialised services have been created. Using low-level services, users have to set up all aspects of the environment for their particular application, choosing appropriate configurations for sharing, security, reliability etc. Usually this means specialised software engineers or infrastructure engineers to make this happen. For a large organisation, there are sufficient people and skills to maintain the goal of separation of concerns, but this is not true for smaller groups and organisations. Instead, one can use higher level services where the technical details are taken care of. Increasingly higher-level service components, sch as data warehousing and machine learning platforms where low-level configuration is taken care of, are being part of the software stack for Pangeo implementations. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Platform_as_a_service\n",
    "\n",
    "Service / Provider | AWS | Azure | GCP \n",
    "--- |--- |--- |---  \n",
    "Machine Learning | Sagemaker | AzureML | DataLab / Cloud AutoML\n",
    "Database | RDS | Azure SQQL DB | Cloud SQL\n",
    "Data Warehouse | \n",
    "Query aaS | Athena | Data Lake Analytics | BigQuery\n",
    "\n",
    "We also have 3rd party providers of these higher level services, building value-added layers on the low-level infrastrucutre of major CSPs to deliver specialised services, for example database solutions ([TileDB](https://tiledb.com/) or [MongoDB](https://www.mongodb.com/cloud))  or machine learning platforms ([Determined AI](https://determined.ai/enterprise/)\n",
    "\n",
    "CSPs liken the development of cloud computing to the development of an integrated grid for electricity distribution. In the early days of electricity each factory had their own generators and required expertise in electrical engineering. With a electricity grid, central suppliers provide the electricity and the associated expertise to run it. This is the direction that computing is going in. The trade-off is that higher level services often are less portable resulting in vendor lock-in. So we balance the convenience of higher-level services in our Pangeo implementation with the goals of reproducible, shareable research which favour open-source tools deployed on low-level services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-compound",
   "metadata": {},
   "source": [
    "### More information on cloud providers\n",
    "\n",
    "* AWS\n",
    "* Azure\n",
    "* Digital Ocean\n",
    "\n",
    "Comparison of cloud providers: https://www.varonis.com/blog/aws-vs-azure-vs-google/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-avatar",
   "metadata": {},
   "source": [
    "## Creating and sharing the tool stack\n",
    "\n",
    "One of the challenges of computing platforms is setting up the right environment of tools and libraries to support the scientific research being done, while avoiding this task consuming all the researcher's time. There have been substantial developments in this space which make this task easier and support the goals of reproducible and shareable research and aid in separation of concerns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-charleston",
   "metadata": {},
   "source": [
    "### Environment managers - pip and conda\n",
    "\n",
    "Particularly in the python ecosystem, tools such as *pip* and *conda* allow one to specify the tools to deploy on a particular compute instance as a file, allowing an **infrastructure as code** (IaC) approach to tools.  As with cloud resource provision, complete specification of the configuration as a file allows others to reproduce the environment and thus reproduce the scientific research. This is not always yet as easy and trouble free as we would hope, but these tools have gone a long way towards this goal and are often used as part of a Pangeo implemntation to configure the research environment.\n",
    "\n",
    "Additional info\n",
    "* pip https://pip.pypa.io/en/stable/\n",
    "* conda https://docs.conda.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-costume",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n",
    "Another similar tool is *containers*. These are essentially lightweight virtual machines intended for running a single task efficiently and at scale. As with environemnt managers, you completely configure an *image* through a cofiguration which specifies what should be installed and configured inside the container. You then build a particular *instance* of your container from the image. One can build hundred of instances to run in parallel. Compute jobs can then be distirbuted among these containers at a task or distribution level, to make use of the massively parallel, distributed nature of the compute and data storage infrastructure. Over time, repositories of ready made containers have built up, so a researcher should not need to do much configuration to get started.\n",
    "\n",
    "\n",
    "\n",
    "Additional information\n",
    "* docker: library for creating and running containers - https://docker.com \n",
    "* Docker Hub: library of ready to use containers - https://hub.docker.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-administrator",
   "metadata": {},
   "source": [
    "## Orchestration\n",
    "\n",
    "The challenge in distributed computing is always getting the many individual workers to coordinate the work they are doing. Before they start doing any actual work, the cluster of workers must be set up appropriately from the cloud resourced we have requested to enable this inter-task communication. This is the job of orchestration software. Once again, we use an Infrastructure as code apporach to specify how many workers we want and how they should be configured and the orchestration software then acquires and sets up the resources, such as cloud VMs running containers.\n",
    "\n",
    "Additional info:\n",
    "* Kubernetes - https://kubernetes.io/\n",
    "* AWS Elastic Container Service - https://aws.amazon.com/ecs/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc\n",
    "* Azure Container services - https://azure.microsoft.com/en-gb/product-categories/containers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-truck",
   "metadata": {},
   "source": [
    "## Task Distribution\n",
    "\n",
    "With our compute cluster set up and running, we then need a way to execute our tasks in a distributed fashion. We need a way to handling splitting our large dataset into sub-domains where a particular operation can be performed separately on each sub-domain or a separate compute worker. One library that does this is dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-monkey",
   "metadata": {},
   "source": [
    "Dask is a task scheduling library which support **lazy execution**. This means that it doesn't actually do any calculations until it needs to. So when you string together a series of operations, for example \n",
    "\n",
    "* load data\n",
    "* extract subset for a country or region\n",
    "* calculate mean for each year for country\n",
    "* plot annual means\n",
    "\n",
    "The calculation will only be triggered when you try to plot the data, as it then needs the actual number. Before that point it creates a [*task graph*](https://docs.dask.org/en/latest/graphs.html), describing all the tasks that need to computed and which tasks are dependant on other tasks. When it decides it needs the results, all of the elements in the graph are calcualted in the order required by the dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-corrections",
   "metadata": {},
   "source": [
    "How does it do this is a massively parallel way to speed up execution? There are three parts to the dask compute resources\n",
    "\n",
    "* a client - usually the computer we are interacting on \n",
    "* a scheduler - the instance the divides up the task and communicates with the workers\n",
    "* workers - compute instances doing the actual work sent to them by the scheduler.\n",
    "\n",
    "When computation is triggered, the cheduler figures out how to assign jobs to workers in the correct order according to the graphm, and then gets the results back from the workers. The task graph will split up a large array by chunk, so each calculation of a chunk of data is a separate node in the graph, which will go to a separate worker. This allows for massive elastic, scaling of compute organised interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-heath",
   "metadata": {},
   "source": [
    "Additional Information:\n",
    "* dask -https://dask.org/\n",
    "* dask distributed - https://distributed.dask.org/en/latest/\n",
    "* dask cloud provider https://cloudprovider.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-portrait",
   "metadata": {},
   "source": [
    "## Distributed computing with dask - A Demonstration\n",
    "\n",
    "At last we come to a demonstration of actually computing with actual source code. This will show how we can do a fairly simple mean calculationm on a large array. This seem quite simple, but is similar to many of the questions we want to ask, which are string together a series of simple operation, often something subset by time and location, calculate mean,min and max. The challenge is doing so an an ensemble of global climate predictions for 100 years! So what does this look like on dask.\n",
    "\n",
    "We start by creating a client object, which also creates connected scheduler and worker objects. We're creating this locally, but this could be connected to a cluster on any sort of infrastructure:\n",
    "* local machine\n",
    "* cloud cluster\n",
    "* on-premises cluster\n",
    "* HPC\n",
    "\n",
    "The cluster absracts away the details of the implemtations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proved-appreciation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen.haddad/opt/anaconda3/envs/pangeo-lectures-env/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 58193 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "import dask.distributed\n",
    "client = dask.distributed.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-motorcycle",
   "metadata": {},
   "source": [
    "Our client shows some details and provides us with a dashboard we can look at, which for a local cluster is at `localhost:8787/status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accepting-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:58194</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:58193/status' target='_blank'>http://127.0.0.1:58193/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:58194' processes=4 threads=8, memory=17.18 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-mechanics",
   "metadata": {},
   "source": [
    "Now we can set up our computation. In this case we want to find the mean of an array. To use dask for our computation, we use a dask array data structure rtather than a standard numpy array. Dask aims to present the same interface for major data type, for example\n",
    "* numpy array - dask array\n",
    "* pandas dataframe - dask dataframe\n",
    "\n",
    "You can also manually create agraph through creating *delayed* functions through the dask API, where normal python functions are added to a task graph to be executed later. Here we are using the dask data types, and this will construct our task graph for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "social-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-going",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000294164532435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array = dask.array.random.random((1000, 1000), chunks=(100, 100))\n",
    "my_array.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-sally",
   "metadata": {},
   "source": [
    "![Dask Task Graph](images/dask_taskGraph.png)\n",
    "This is what the graph looks like for our small operation. Each chunk is a node in the graph, and then the scheduler gathers together the result to present in our notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-category",
   "metadata": {},
   "source": [
    "## Interactivity & portability - Jupyter Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-region",
   "metadata": {},
   "source": [
    "Introduction to Jupyer labs\n",
    "\n",
    "(use material from pythobn guild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo running compute (again)\n",
    "#demo accessing data in notebook\n",
    "# demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-twins",
   "metadata": {},
   "source": [
    "## Dashboarding\n",
    "\n",
    "What is a dashboard\n",
    "introduction to bokeh for interactive visualisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instructional-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo bokeh, holoview in a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-terrorism",
   "metadata": {},
   "source": [
    "## Existing example installations \n",
    "\n",
    "Pangeo has been installed in many different places around the world\n",
    "* Informatics Lab research deployment - AWS, Azure\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-cleveland",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "finite-passport",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "If you want to set up your own Pangeo instance, the Pangeo community has lots of different recipes and examples for doing, available through the Pangeo Community Website, and community help available through Discourse:\n",
    "\n",
    "* Deployment guide: https://pangeo.io/setup_guides/index.html\n",
    "* P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-inclusion",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-legislation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
